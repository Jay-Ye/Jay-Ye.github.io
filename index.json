
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"üçÄ I am a first-year CS PhD student at USC, advised by Prof. Yue Wang. I obtained my bachelor‚Äôs and master‚Äôs degrees from Tongji University with the highest honor Academic Pioneer, where I worked in Vision4robotics Group directed by Prof. Changhong Fu. I have been fortunate to spend time at CVG Group in ETHz (under ETH RSF 2022), and MARS Lab in QiZhi Institute.\nDownload my Resum√©.\n Download my resum√©. --  -- ","date":1689292800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1689292800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"üçÄ I am a first-year CS PhD student at USC, advised by Prof. Yue Wang. I obtained my bachelor‚Äôs and master‚Äôs degrees from Tongji University with the highest honor Academic Pioneer, where I worked in Vision4robotics Group directed by Prof.","tags":null,"title":"Junjie Ye","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy‚Äôs Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://example.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Bowen Li","Ziyuan Huang","Junjie Ye","Yiming Li","Sebastian Scherer","Hang Zhao","Changhong Fu"],"categories":null,"content":"","date":1689292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689292800,"objectID":"da136e957cc45d87b86a21fda225f0fd","permalink":"https://example.com/publication/2023_iccv_pvt++/","publishdate":"2023-07-14T00:00:00Z","relpermalink":"/publication/2023_iccv_pvt++/","section":"publication","summary":"ICCV2023. *A simple and effective framework to realize robust predictive visual tracking.*","tags":["Unmanned aerial vehicle","Visual object tracking","Latency-aware tracking"],"title":"PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework","type":"publication"},{"authors":["Changhong Fu","Kunhan Lu","Guangze Zheng","Junjie Ye","Ziang Cao","Bowen Li","Geng Lu"],"categories":null,"content":"","date":1688774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688774400,"objectID":"363b7ad99a9cb7701e9a38a1dca922ad","permalink":"https://example.com/publication/2022_air_siam/","publishdate":"2023-07-08T00:00:00Z","relpermalink":"/publication/2022_air_siam/","section":"publication","summary":"Unmanned aerial vehicle (UAV)-based visual object tracking has enabled a wide range of applications and attracted increasing attention in the field of remote sensing because of its versatility and effectiveness. As a new force in the revolutionary trend of deep learning, Siamese networks shine in visual object tracking with their promising balance of accuracy, robustness, and speed. Thanks to the development of embedded processors and the gradual optimization of deep neural networks, Siamese trackers receive extensive research and realize preliminary combinations with UAVs. However, due to the UAV's limited onboard computational resources and the complex real-world circumstances, aerial tracking with Siamese networks still faces severe obstacles in many aspects. To further explore the deployment of Siamese networks in UAV tracking, this work presents a comprehensive review of leading-edge Siamese trackers, along with an exhaustive UAV-specific analysis based on the evaluation using a typical UAV onboard processor. Then, the onboard tests are conducted to validate the feasibility and efficacy of representative Siamese trackers in real-world UAV deployment. Furthermore, to better promote the development of the tracking community, this work analyzes the limitations of existing Siamese trackers and conducts additional experiments represented by low-illumination evaluations. In the end, prospects for the development of Siamese UAV tracking in the remote sensing field are discussed. The unified framework of leading-edge Siamese trackers, i.e., code library, and the results of their experimental evaluations are available at https://github.com/vision4robotics/SiameseTracking4UAV.","tags":["Unmanned aerial vehicle (UAV)","Vision-based aerial object tracking","Siamese networks","Review \u0026 comprehensive analysis"],"title":"Siamese Object Tracking for Unmanned Aerial Vehicle: A Review and Comprehensive Analysis","type":"publication"},{"authors":["Changhong Fu","Teng Li","Junjie Ye","Guangze Zheng","Sihang Li","Peng Lu"],"categories":null,"content":"","date":1682640000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682640000,"objectID":"d8f42a9bd4cea26acf0327fc6230fe64","permalink":"https://example.com/publication/2023_ral_sada/","publishdate":"2023-04-28T00:00:00Z","relpermalink":"/publication/2023_ral_sada/","section":"publication","summary":"Siamese object tracking has facilitated diversified applications for autonomous unmanned aerial vehicles (UAVs). However, they are typically trained on general images with relatively large objects instead of small objects observed from UAV. The gap on object scale between the training and inference phases is prone to suboptimal tracking performance or even failure. To solve the gap issue and tailor general Siamese trackers for UAV tracking, this work proposes a novel scale-aware domain adaptation framework, i.e., ScaleAwareDA. Specifically, a contrastive learning-inspired network is proposed to guide the training phase of Transformer-based feature alignment for small objects. In this network, feature projection module is designed to avoid information loss of small objects. Feature prediction module is developed to drive the aforementioned training phase in a self-supervised way. In addition, to construct the target domain, training datasets with UAV-specific attributes are obtained by downsampling general training datasets. Consequently, this novel training approach can assist a tracker to represent objects in UAV scenarios more powerfully and thus maintain its robustness. Extensive experiments on three authoritative challenging UAV tracking benchmarks have demonstrated the superior tracking performance of ScaleAwareDA. In addition, quantitative real-world tests further attest to its practicality.","tags":["Unmanned aerial vehicle","Visual object tracking","Transformer-based domain adaptation","Contrastive learning-inspired training"],"title":"Scale-Aware Domain Adaptation for Robust UAV Tracking","type":"publication"},{"authors":["Liangliang Yao","Changhong Fu","Sihang Li","Guangze Zheng","Junjie Ye"],"categories":null,"content":"\r   Overall comparison of tracking results between baseline (blue) and the proposed saliency-guided dynamic vision Transformer (SGDViT) (red).\n\r","date":1673769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673769600,"objectID":"2268e9ba32c801ec736fd13214dcb1c4","permalink":"https://example.com/publication/2023_icra_sgdvit/","publishdate":"2023-01-15T08:00:00Z","relpermalink":"/publication/2023_icra_sgdvit/","section":"publication","summary":"Vision-based object tracking has boosted extensive autonomous applications for unmanned aerial vehicles (UAVs). However, the frequent maneuvering flight and viewpoint change are prone to cause nerve-wracking challenges, e.g., aspect ratio change and scale variation. The cross-correlation operation‚Äôs weak ability to mine perceptual similarity and easy introduction of background information become more apparent when confronted with these challenges. To address these issues, this work proposes a novel saliency-guided dynamic vision Transformer (SGDViT) for UAV tracking. Specifically, a UAV taskoriented object saliency mining network is designed to refine the perceptual similarity indicated by cross-correlation operation, distinguishing the foreground and background preliminarily. Furthermore, an innovative saliency adaption embedding operation is developed to generate dynamic tokens based on the initial saliency, reducing the computational complexity of the Transformer structure. On this bases, a lightweight saliency filtering Transformer is implemented to refine the saliency information and increase attention to the appearance information. Comprehensive evaluations on three authoritative UAV tracking benchmarks and real-world tests have proven the effectiveness and robustness of the proposed method. The source code and demo videos are available at https://github.com/vision4robotics/SGDViT.","tags":["Visual tracking","Unmanned aerial vehicles","Saliency-guided dynamic Transformer"],"title":"SGDViT: Saliency-Guided Dynamic Vision Transformer for UAV Tracking","type":"publication"},{"authors":["Guangze Zheng","Changhong Fu","Junjie Ye","Bowen Li","Geng Lu","Jia Pan"],"categories":null,"content":"     Demonstration of the vision-based UAM approaching system and qualitative comparison.\n","date":1669420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669420800,"objectID":"b955eef0b8aa0a18c8a9855b7efb2ddb","permalink":"https://example.com/publication/2022_tii_siamsa/","publishdate":"2022-11-26T00:00:00Z","relpermalink":"/publication/2022_tii_siamsa/","section":"publication","summary":"In many industrial applications of unmanned aerial manipulator (UAM), visual approaching to the object is crucial to subsequent manipulating. In comparison with the widely-studied manipulating, the key to efficient vision-based UAM approaching, i.e., UAM object tracking, is still limited. Since traditional model-based UAM tracking is costly and cannot track arbitrary objects, an intuitive solution is to introduce state-of-the-art model-free Siamese trackers from the visual tracking field. Although Siamese tracking is most suitable for the onboard embedded processors, severe object scale variation in UAM tracking brings formidable challenges. To address these problems, this work proposes a novel model-free scale-aware Siamese tracker (SiamSA). Specifically, a scale attention network is proposed to emphasize scale awareness in feature processing. A scale-aware anchor proposal network is designed to achieve anchor proposing. Besides, two novel UAM tracking benchmarks are first recorded. Comprehensive experiments on benchmarks validate the effectiveness of SiamSA. Furthermore, real-world tests also confirm practicality for industrial UAM approaching tasks with high efficiency and robustness.","tags":["Unmanned aerial manipulator","UAM approaching","Visual tracking"],"title":"Scale-Aware Siamese Object Tracking for Vision-Based UAM Approaching","type":"publication"},{"authors":["Haobo Zuo","Changhong Fu","Sihang Li","Junjie Ye","Guangze Zheng"],"categories":null,"content":"    Overview of the proposed DeconNet.\n","date":1665619200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665619200,"objectID":"55d94224963f968019c289fda46116f7","permalink":"https://example.com/publication/2022_tgrs_deconnet/","publishdate":"2022-10-13T00:00:00Z","relpermalink":"/publication/2022_tgrs_deconnet/","section":"publication","summary":"Vision-based aerial tracking has proven enormous potential in the field of remote sensing recently. However, challenges such as occlusion, fast motion, and illumination variation remain crucial issues for realistic aerial tracking applications. These challenges, frequently occurring from the aerial perspectives, can easily cause object feature pollution. With the contaminated object features, the credibility of trackers is prone to be substantially degraded. To address this issue, this work proposes a novel end-to-end decontaminated network, i.e., DeconNet, to alleviate object feature pollution efficiently and effectively. DeconNet mainly consists of downsampling and upsampling phases. Specifically, the decontaminated downsampling network first decreases the polluted object information with two convolution branches, enhancing the object location information. Subsequently, the decontaminated upsampling network applies the super-resolution technology to restore the object scale and shape information, with the low-to-high encoder for further decontamination. Additionally, the pooling distance loss function is carefully designed to improve the decontamination effect of the decontaminated downsampling network. Comprehensive evaluations on four well-known aerial tracking benchmarks validate the effectiveness of DeconNet. Especially, the proposed tracker has superior performance on the sequences with feature pollution. Besides, real-world tests on an aerial platform have proven the efficiency of DeconNet with 30.6 frames per second.","tags":["Vision-based aerial tracking","End-to-end decontaminated network","Downsampling-upsampling strategy","Low-to-high encoder","Pooling distance loss"],"title":"DeconNet: End-to-End Decontaminated Network for Vision-Based Aerial Tracking","type":"publication"},{"authors":["Haobo Zuo","Changhong Fu","Sihang Li","Junjie Ye","Guangze Zheng"],"categories":null,"content":"","date":1656547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656547200,"objectID":"2e5fc7f571f6d05b8e4c4ad27024326c","permalink":"https://example.com/publication/2022_iros_fdnt/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/2022_iros_fdnt/","section":"publication","summary":"Object feature pollution is one of the burning issues in vision-based UAV tracking, commonly caused by occlusion, fast motion, and illumination variation. Due to the contaminated information in the polluted object features, most trackers fail to precisely estimate the object location and scale. To address the above disturbing issue, this work proposes a novel end-to-end feature decontaminated network for efficient and effective UAV tracking, i.e., FDNT. FDNT mainly includes two modules, a decontaminated downsampling network and a decontaminated upsampling network. The former reduces the interference information of the feature pollution and enhanced the expression of the object location information with two asymmetric convolution branches. The latter restores the object scale information with the super-resolution technology-based low-to-high encoder, achieving a further decontamination effect. Moreover, a novel pooling distance loss is carefully developed to assist the decontaminated downsampling network in concentrating on the critical regions with the object information. Exhaustive experiments on three well-known benchmarks validate the effectiveness of FDNT, especially on the sequences with feature pollution. In addition, real-world tests show the efficiency of FDNT with 31.4 frames per second. The code and demo videos are available at https://github.com/vision4robotics/FDNT.","tags":["Feature Decontamination","UAV tracking"],"title":"End-to-End Feature Decontaminated Network for UAV Tracking","type":"publication"},{"authors":["Changhong Fu","Haolin Dong","Junjie Ye","Guangze Zheng","Sihang Li","Jilin Zhao"],"categories":null,"content":"","date":1656547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656547200,"objectID":"eef60c6bb8fef1ccbd07cdb4c063a45c","permalink":"https://example.com/publication/2022_iros_highlightnet/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/2022_iros_highlightnet/","section":"publication","summary":"Low-light environments have posed a formidable challenge for robust UAV tracking even with state-of-the-art trackers since the potential image features are hard to extract under adverse light conditions. Besides, due to the low visibility, accurate online selection of the object also becomes extremely difficult for human monitors to initialize UAV tracking in ground control stations (GCSs). To address these problems, this work proposed a novel enhancer, i.e., HighlightNet, to light up potential objects for both human operators and UAV trackers. By employing Transformer, HighlightNet can adjust enhancement parameters according to global features and is thus adaptive for illumination variation. Pixel-level range mask is introduced to make HighlightNet more focused on the enhancement of the tracking object and regions without light sources. Furthermore, a soft truncation mechanism is built to prevent background noise from being mistaken for crucial features. Experiments on image enhancement benchmarks demonstrate HighlightNet has advantages in facilitating human perception. Evaluations on the public UAVDark135 benchmark show that HightlightNet is more suitable for UAV tracking tasks than other top-ranked low-light enhancers. In addition, with real-world tests on a typical UAV platform, HighlightNet verifies its practicability and efficiency in nighttime aerial tracking-related applications. The code and demo videos are available at https://github.com/vision4robotics/HighlightNet.","tags":["Nighttime UAV tracking","Low-light enhancement"],"title":"HighlightNet: Highlighting Low-Light Potential Features for Real-Time UAV Tracking","type":"publication"},{"authors":["Changhong Fu","Weiyu Peng","Sihang Li","Junjie Ye","Ziang Cao"],"categories":null,"content":"","date":1656547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656547200,"objectID":"71a6432aeaf6f6ee4a6a6346797058e0","permalink":"https://example.com/publication/2022_iros_lpat/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/2022_iros_lpat/","section":"publication","summary":"Visual object tracking has been utilized in numerous aerial platforms, where is facing the challenges of more extremely complex conditions. To address the inefficient long-range modeling of traditional networks with fully convolutional neural networks, Transformer is introduced into the state-of-the-art trackers‚Äô frameworks. Benefiting from full receptive field of global attention, these Transformer trackers can efficiently model long-range information. However, the structure of vanilla Transformer is lack of enough inductive bias and directly adopting global attention will lead to overfocusing on global information which does harm to modeling local details. This work proposes a local perception-aware Transformer for aerial tracking, i.e., LPAT. Specifically, this novel tracker is constructed with modified local-recognition attention and local element correction network to process information via local-modeling to global-search mechanism. To grab local details and strengthen the local inductive bias of Transformer structure. The Transformer encoder with local-recognition attention is constructed to fuse local features for accurate feature modeling and the local element correction network can strengthen the capability of both Transformer encoder and decoder to distinguish local details. The proposed method achieves competitive accuracy and robustness in several benchmarks with 316 sequences in total. The proposed tracker‚Äôs practicability and efficiency have been validated by the real-world tests on a typical aerial platform. The code is available at https://github.com/vision4robotics/LPAT.","tags":["Aerial object tracking","Local perception-aware Transformer"],"title":"Local Perception-Aware Transformer for Aerial Tracking","type":"publication"},{"authors":["Guangze Zheng","Changhong Fu","Junjie Ye","Bowen Li","Geng Lu","Jia Pan"],"categories":null,"content":"","date":1656547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656547200,"objectID":"778857e5a8a94eb84cf3c8f49d6491fc","permalink":"https://example.com/publication/2022_iros_siamsa/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/publication/2022_iros_siamsa/","section":"publication","summary":"Visual approaching to the target object is crucial to the subsequent manipulating of the unmanned aerial manipulator (UAM). Although the manipulating methods have been widely studied, the vision-based UAM approaching generally lacks efficient design. The key to the visual UAM approaching lies in object tracking, while current approaching generally relies on costly model-based methods. Besides, UAM approaching often confronts more severe object scale variation issues, which makes it inappropriate to directly employ state-of-the-art model-free Siamese-based methods from the object tracking field. To address the above problems, this work proposes a novel Siamese network with pairwise scale-channel attention (SiamPSA) for vision-based UAM approaching. Specifically, SiamPSA consists of a scale attention network (SAN) and a scale-aware anchor proposal network (SA-APN). SAN acquires valuable scale information for feature processing, while SA-APN mainly attaches scale-awareness to anchor proposing. Moreover, a new tracking benchmark for UAM approaching, namely UAMT100, is recorded with 35K frames on a flying UAM platform for evaluation. Exhaustive experiments on the benchmark and real-world tests validate the efficiency and practicality of SiamPSA with a promising speed. Both the code and UAMT100 benchmark are now available at https://github.com/vision4robotics/SiamSA.","tags":["Unmanned aerial manipulator","UAM approaching","Visual tracking"],"title":"Siamese Object Tracking for Vision-Based UAM Approaching with Pairwise Scale-Channel Attention","type":"publication"},{"authors":["Shan An","Haogang Zhu","Jiaao Zhang","Junjie Ye","Siliang Wang","Jianqin Yin","Hong Zhang"],"categories":null,"content":"","date":1655424000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655424000,"objectID":"7c92c24a289590d1aa8bdbce6ba71906","permalink":"https://example.com/publication/2022_ral_dtt/","publishdate":"2022-06-17T00:00:00Z","relpermalink":"/publication/2022_ral_dtt/","section":"publication","summary":"Semantic segmentation is of great value to autonomous driving and many robotic applications, while it highly depends on costly and time-consuming pixel-level annotation. To make full use of unlabeled data, this work proposes a deep tri-training framework (dubbed DTT) to utilize labeled along with unlabeled data for training in a semi-supervised manner. Concretely, in the DTT framework, three networks are initialized with the same structure but different parameters. The networks are optimized circularly, where one network is trained in each optimization step with the guidance of the other two networks. A simple yet effective voting mechanism is adopted to construct reliable training sets from unlabeled data for the training stage and fusing multi-experts prediction in the testing stage. Exhaustive experiments on Cityscapes and PASCAL VOC 2012 demonstrate that the proposed DTT realizes state-of-the-art performance in the semi-supervised segmentation task. The source code is available in the supplementary material and will be made publicly available.","tags":["Object Detection","Segmentation and Categorization","Semantic Scene Understanding","Deep learning for visual perception","Deep Learning Methods"],"title":"Deep Tri-Training for Semi-Supervised Image Segmentation","type":"publication"},{"authors":["Bowen Li","Changhong Fu","Fangqiang Ding","Junjie Ye","Fuling Lin"],"categories":null,"content":"","date":1647907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647907200,"objectID":"f4f390c57691f15cf24a00c711c64ce9","permalink":"https://example.com/publication/2022_tmc_adtrack/","publishdate":"2022-03-22T00:00:00Z","relpermalink":"/publication/2022_tmc_adtrack/","section":"publication","summary":"Unmanned aerial vehicle (UAV) has facilitated a wide range of real-world applications and attracted extensive research in the mobile computing field. Specially, developing real-time robust visual onboard trackers for all-day aerial maneuver can remarkably broaden the scope of intelligent deployment of UAV. However, prior tracking methods have merely focused on robust tracking in the well-illuminated scenes, while ignoring trackers‚Äô capabilities to be deployed in the dark. In darkness, the conditions can be more complex and harsh, easily posing inferior robust tracking or even tracking failure. To this end, this work proposes a novel discriminative correlation filter-based tracker with illumination adaptive and anti-dark capability, namely ADTrack. ADTrack firstly exploits image illuminance information to enable adaptability of the model to the given light condition. Then, by virtue of an efficient enhancer, ADTrack carries out image pretreatment where a target aware mask is generated. Benefiting from the mask, ADTrack aims to solve a novel dual regression problem where dual filters are online trained with mutual constraint. Besides, this work also constructs a UAV nighttime tracking benchmark UAVDark135. Exhaustive experiments on authoritative benchmarks and onboard tests are implemented to validate the superiority and robustness of ADTrack in all-day conditions.","tags":["Unmanned aerial vehicle","Visual object tracking","Discriminative correlation filter","Dark tracking benchmark","Image illumination based mask","Dual regression model"],"title":"All-Day Object Tracking for Unmanned Aerial Vehicle","type":"publication"},{"authors":["Junjie Ye","Changhong Fu","Guangze Zheng","Danda Pani Paudel","Guang Chen"],"categories":null,"content":"","date":1646179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646179200,"objectID":"10d2a58c9f73c59e61d4a01ed1b791b3","permalink":"https://example.com/publication/2022_cvpr_udat/","publishdate":"2022-03-01T00:00:00Z","relpermalink":"/publication/2022_cvpr_udat/","section":"publication","summary":"CVPR2022. *Proposed an unsupervised domain adaptation framework to adapt object tracking from daytime to nighttime, along with a nighttime tracking benchmark.*","tags":["Unsupervised domain adaptation","Nighttime aerial tracking","Benchmark","Transformer"],"title":"Unsupervised Domain Adaptation for Nighttime Aerial Tracking","type":"publication"},{"authors":["Changhong Fu","Sihang Li","Xinnan Yuan","Junjie Ye","Ziang Cao","Fangqiang Ding"],"categories":null,"content":"","date":1643702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643702400,"objectID":"4c886f02ab35d2f45fd991574daea621","permalink":"https://example.com/publication/2022_icra_ad2attack/","publishdate":"2022-01-31T08:00:00Z","relpermalink":"/publication/2022_icra_ad2attack/","section":"publication","summary":"Visual tracking is adopted to extensive unmanned aerial vehicle (UAV)-related applications, which leads to a highly demanding requirement on the robustness of UAV trackers. However, adding imperceptible perturbations can easily fool the tracker and cause tracking failures. This risk is often overlooked and rarely researched at present. Therefore, to help increase awareness of the potential risk and the robustness of UAV tracking, this work proposes a novel adaptive adversarial attack approach, i.e., Ad2Attack, against UAV object tracking. Specifically, adversarial examples are generated online during the resampling of the search patch image, which leads trackers to lose the target in the following frames. Ad2Attack is composed of a direct downsampling module and a super-resolution upsampling module with adaptive stages. A novel optimization function is proposed for balancing the imperceptibility and efficiency of the attack. Comprehensive experiments on several well-known benchmarks and real-world conditions show the effectiveness of our attack method, which dramatically reduces the performance of the most advanced Siamese trackers.","tags":["Visual tracking","Unmanned aerial vehicles","Adversarial attack"],"title":"Ad2Attack: Adaptive Adversarial Attack on Real-Time UAV Tracking","type":"publication"},{"authors":["Junjie Ye","Changhong Fu","Ziang Cao","Shan An","Guangze Zheng","Bowen Li"],"categories":null,"content":"","date":1641168000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641168000,"objectID":"e9308ae6e15c9f7ab24895829b29945f","permalink":"https://example.com/publication/2022_ral_sct/","publishdate":"2022-01-03T00:00:00Z","relpermalink":"/publication/2022_ral_sct/","section":"publication","summary":"IEEE RA-L with ICRA2022. *Trained a spatial-channel transformer-based low-light enhancer in a novel task-related manner, to facilitate nighttime aerial tracking significantly.*","tags":["Unmanned aerial vehicle","Nighttime tracking","Low-light enhancement","Transformer"],"title":"Tracker Meets Night: A Transformer Enhancer for UAV Tracking","type":"publication"},{"authors":null,"categories":null,"content":"","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"a3ca2af2b6970d3faedb710a28839ba4","permalink":"https://example.com/project/darktrack2021/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/project/darktrack2021/","section":"project","summary":"DarkTrack2021 is a nighttime tracking benchmark comprises 110 challenging sequences with 100K frames in total.","tags":["UAV dark tracking","Benchmark"],"title":"DarkTrack2021","type":"project"},{"authors":null,"categories":null,"content":"","date":1632700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632700800,"objectID":"7c217d7a0d9af29d165dd79e76d663fe","permalink":"https://example.com/project/uamt100/","publishdate":"2021-09-27T00:00:00Z","relpermalink":"/project/uamt100/","section":"project","summary":"UAMT100 benchmark is built for UAM tracking method evaluation. It contains 100 image sequences recorded on a flying UAM platform.","tags":["UAM tracking","Benchmark"],"title":"UAMT100","type":"project"},{"authors":["Shan An","Guangfu Che","Jinghao Guo","Haogang Zhu","Junjie Ye","Fangru Zhou","Zhaoqi Zhu","Dong Wei","Aishan Liu","Wei Zhang"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"1fe5325a2cdf2c9c0b7312d0f5c96041","permalink":"https://example.com/publication/2021_acmmm_arshoe/","publishdate":"2021-08-01T00:00:00Z","relpermalink":"/publication/2021_acmmm_arshoe/","section":"publication","summary":"ACM MM2021. *Proposed a real-time augmented reality virtual shoe try-on system for smartphones. The system has been implemented in [JD App](https://app.jd.com/).*","tags":["Augmented reality","Visual shoe try-on"],"title":"ARShoe: Real-Time Augmented Reality Shoe Try-on System on Smartphones","type":"publication"},{"authors":["Ziang Cao","Changhong Fu","Junjie Ye","Bowen Li","Yiming Li"],"categories":null,"content":"","date":1626912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626912000,"objectID":"c9ab8c003b230e357d009edabed830f2","permalink":"https://example.com/publication/2021_iccv_hift/","publishdate":"2019-07-22T00:00:00Z","relpermalink":"/publication/2021_iccv_hift/","section":"publication","summary":"ICCV2021. *Introduced the hierarchical feature transformer into the Siamese framework to achieve interactive fusion of spatial and semantic cues.*","tags":["Siamese network","Real-time object tracking","Unmanned aerial vehicles","Transformer"],"title":"HiFT: Hierarchical Feature Transformer for Aerial Tracking","type":"publication"},{"authors":["Junjie Ye","Changhong Fu","Guangze Zheng","Ziang Cao","Bowen Li"],"categories":null,"content":"","date":1625011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625011200,"objectID":"c98ba9d164d6dc005d7f59a9f3dff2f1","permalink":"https://example.com/publication/2021_iros_darklighter/","publishdate":"2021-06-30T00:00:00Z","relpermalink":"/publication/2021_iros_darklighter/","section":"publication","summary":"IROS2021. *Designed a Retinex-inspired plug-and-play deep low-light enhancer to light up the darkness for UAV tracking.*","tags":["Low-light enhancement","Visual tracking","Unmanned aerial vehicles"],"title":"DarkLighter: Light Up the Darkness for UAV Tracking","type":"publication"},{"authors":["Ziang Cao","Changhong Fu","Junjie Ye","Bowen Li","Yiming Li"],"categories":null,"content":"","date":1625011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625011200,"objectID":"0c32adec2cf596405b8de13633f93db3","permalink":"https://example.com/publication/2021_iros_siamapn++/","publishdate":"2021-06-30T00:00:00Z","relpermalink":"/publication/2021_iros_siamapn++/","section":"publication","summary":"**IROS2021.** *Integrated self-attention and cross-attention into the Siamese framework to enhanced the perception ability for various scale objects.*","tags":["Siamese network","Visual tracking","Unmanned aerial vehicles"],"title":"SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV Tracking","type":"publication"},{"authors":["Junjie Ye","Changhong Fu","Fuling Lin","Fangqiang Ding","Shan An","Geng Lu"],"categories":null,"content":"","date":1621814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621814400,"objectID":"2dc6df5d1fe73a4908c6864fe515e2ba","permalink":"https://example.com/publication/2021_tie_mrcf_tracker/","publishdate":"2021-05-24T00:00:00Z","relpermalink":"/publication/2021_tie_mrcf_tracker/","section":"publication","summary":"IEEE T-IE (IF: 7.7). *Proposed the multi-regularized CF and constructed a visual tracking-based UAV self-localization system.*","tags":["Unmanned aerial vehicle (UAV)","Model-free object tracking","Multi-regularized correlation filter","Vision-based UAV self-localization"],"title":"Multi-Regularized Correlation Filter for UAV Tracking and Self-Localization","type":"publication"},{"authors":["Changhong Fu","Ziang Cao","Yiming Li","Junjie Ye","Chen Feng"],"categories":null,"content":"    The workflow of SiamAPN. It is composed of four subnetworks and two stages, i.e., feature extraction network, feature fusion network, anchor proposal network, and classification\u0026amp;regression network. Stage-1 includes feature extraction network and anchor proposal network (APN). Stage-2 contains feature fusion network and classification\u0026amp;regression network.\n","date":1620950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620950400,"objectID":"f33f3154720a3c4a64b5aa1335e2e34f","permalink":"https://example.com/publication/2021_tgrs_siamapn_ext/","publishdate":"2021-05-14T00:00:00Z","relpermalink":"/publication/2021_tgrs_siamapn_ext/","section":"publication","summary":"Object tracking approaches based on siamese network have demonstrated their huge potential in remote sensing field recently. Nevertheless, due to the limited computing resource of aerial platforms and special challenges in aerial tracking, most existing siamese-based methods can hardly meet the real-time and state-of-the-art performance at the same time. Consequently, a novel siamese-based method is proposed in this work for onboard real-time aerial tracking, i.e., SiamAPN. The proposed method is a no-prior two-stage method, i.e., stage-1 for proposing adaptive anchors to enhance the ability of object perception, stage-2 for fine-tuning the proposed anchors to obtain accurate results. Distinct from pre-defined fixed-sized anchors, our adaptive anchors are adapt automatically to accommodate the tracking object. Besides, the internal information of adaptive anchors is utilized to feedback SiamAPN for enhancing the object perception. Attributing to the feature fusion network, different semantic information is integrated, enriching the information flow. In the end, the regression and multi-classification operation refine the proposed anchors meticulously. Comprehensive evaluations on three well-known benchmarks have proven the superior performance of our approach. Moreover, to verify the practicability of the proposed method, SiamAPN is implemented in an onboard system. Real-world flight tests are conducted on aerial tracking specific scenarios, e.g., low resolution, fast motion, and long-term tracking, the results demonstrate the efficiency and accuracy of our approach, with a processing speed of over 30 frame/s. In addition, the image sequences in the real-world flight tests are collected and annotated as a new benchmark, i.e., UAVTrack112.","tags":["Real-time aerial tracking","Efficient siamese structure","Anchor proposal network","No-prior adaptive anchors","Onboard embedded processing","Real-world flight tests"],"title":"Onboard Real-Time Aerial Tracking with Efficient Siamese Anchor Proposal Network","type":"publication"},{"authors":["Bowen Li","Yiming Li","Junjie Ye","Changhong Fu","Hang Zhao"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"8f69f97ecf740fb99d1a9fb280cd8166","permalink":"https://example.com/publication/2021_arxiv_pvt/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/publication/2021_arxiv_pvt/","section":"publication","summary":"*A latency-aware benchmark to evaluate tracking methods in a more realistic way.*","tags":["Unmanned aerial vehicle","Visual object tracking","Latency-aware tracking"],"title":"Predictive Visual Tracking: A New Benchmark and Baseline Approach","type":"publication"},{"authors":["Bowen Li","Changhong Fu","Fangqiang Ding","Junjie Ye","Fuling Lin"],"categories":null,"content":"","date":1614499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614499200,"objectID":"1433737504ce4cad2e1d5e6586c8e05f","permalink":"https://example.com/publication/2021_icra_adtrack/","publishdate":"2021-02-28T08:00:00Z","relpermalink":"/publication/2021_icra_adtrack/","section":"publication","summary":"Prior correlation filter (CF)-based tracking methods for unmanned aerial vehicles (UAVs) have virtually focused on tracking in the daytime. However, when the night falls, the trackers will encounter more harsh scenes, which can easily lead to tracking failure. In this regard, this work proposes a novel tracker with anti-dark function (ADTrack). The proposed method integrates an effcient and effective low-light image enhancer into a CF-based tracker. Besides, a target-aware mask is simultaneously generated by virtue of image illumination variation. The target-aware mask can be applied to jointly train a target-focused filter that assists the context filter for robust tracking. Specifically, ADTrack adopts dual regression, where the context filter and the target-focused filter restrict each other for dual filter learning. Exhaustive experiments are conducted on typical dark sceneries benchmark, consisting of 37 typical night sequences from authoritative benchmarks, i.e., UAVDark, and our newly constructed benchmark UAVDark70. The results have shown that ADTrack favorably outperforms other state-of-the-art trackers and achieves a real-time speed of 34 frames/s on a single CPU, thus greatly extending robust UAV tracking to night scenes.","tags":["Visual tracking","Unmanned aerial vehicles","Correlation filter"],"title":"ADTrack: Target-Aware Dual Filter Learning for Real-Time Anti-Dark UAV Tracking","type":"publication"},{"authors":["Guangze Zheng","Changhong Fu","Junjie Ye","Fuling Lin","Fangqiang Ding"],"categories":null,"content":"","date":1614499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614499200,"objectID":"2b51ed03d91fc1ed1d4cc28c2abf7736","permalink":"https://example.com/publication/2021_icra_mscf_tracker/","publishdate":"2021-02-28T08:00:00Z","relpermalink":"/publication/2021_icra_mscf_tracker/","section":"publication","summary":"ICRA2021. *Constructed a novel CF-based tracker to enhance the sensitivity and resistance to mutations with an adaptive hybrid label.*","tags":["Visual tracking","Unmanned aerial vehicles","Correlation filter"],"title":"Mutation Sensitive Correlation Filter for Real-Time UAV Tracking with Adaptive Hybrid Label","type":"publication"},{"authors":["Changhong Fu","Ziang Cao","Yiming Li","Junjie Ye","Chen Feng"],"categories":null,"content":"\r   The overview of SiamAPN tracker. It composes of four subnetworks, i.e., feature extraction network, feature fusion network, anchor proposal network (APN), and muti-classification¬Æression network.\n\r","date":1614499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614499200,"objectID":"9e61feeea128ae22e4769a11f6af92fb","permalink":"https://example.com/publication/2021_icra_siamapn/","publishdate":"2021-02-28T08:00:00Z","relpermalink":"/publication/2021_icra_siamapn/","section":"publication","summary":"In the domain of visual tracking, most deep learning-based trackers highlight the accuracy but casting aside efficiency, thereby impeding their real-world deployment on mobile platforms like the unmanned aerial vehicle (UAV). In this work, a novel two-stage siamese network-based method is proposed for aerial tracking, \\textit{i.e.}, stage-1 for high-quality anchor proposal generation, stage-2 for refining the anchor proposal. Different from anchor-based methods with numerous pre-defined fixed-sized anchors, our no-prior method can 1) make tracker robust and general to different objects with various sizes, especially to small, occluded, and fast-moving objects, under complex scenarios in light of the adaptive anchor generation, 2) make calculation feasible due to the substantial decrease of anchor numbers. In addition, compared to anchor-free methods, our framework has better performance owing to refinement at stage-2. Comprehensive experiments on three benchmarks have proven the state-of-the-art performance of our approach, with a speed of ~ 200 frames/s.","tags":["Visual tracking","Unmanned aerial vehicles","Anchor proposal network"],"title":"Siamese Anchor Proposal Network for High-Speed Aerial Tracking","type":"publication"},{"authors":null,"categories":null,"content":"","date":1614384000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614384000,"objectID":"a8f5650c03497fa0a8aab6c57df848b4","permalink":"https://example.com/project/pvt/","publishdate":"2021-02-27T00:00:00Z","relpermalink":"/project/pvt/","section":"project","summary":"Predictive visual tracking (PVT) is a latency-aware benchmark jointly assessing the tracking accuracy and efficiency of trackers.","tags":["UAV tracking","Latency-aware","Benchmark"],"title":"PVT","type":"project"},{"authors":null,"categories":null,"content":"Overview Exploit low-light enhancement, denoising, domain adaption technologies to adapt SOTA tracking approaches to low-light conditions. With slight computational consumption, enable UAV tracking at night.\n Papers with code Related works are presented as follows:\n  Designed a Retinex-inspired plug-and-play deep low-light enhancer, dubbed DarkLighter, to light up the darkness for UAV tracking. Experiments on a dark tracking benchmark verify its effectiveness in several trackers and superiority against other SOTA general low-light enhancement algorithms, with sufficient real-time speed on an embedded system. \n DarkLighter: Light up the Darkness for UAV Tracking in IROS 2021\n   Constructed a spatial-channel Transformer (SCT) enhancer to facilitate nighttime UAV tracking in a task-inspired manner. Evaluations on the public UAVDark135 and the newly constructed DarkTrack2021 benchmarks demonstrate that the performance gains of SCT brought to nighttime UAV tracking surpass general low-light enhancers. \n Tracker Meets Night: A Transformer Enhancer for UAV Tracking in RA-L with ICRA2022 presentation\n   Proposed an unsupervised domain adaptation framework to adapt object tracking from daytime to nighttime, along with a nighttime tracking benchmark. \n Unsupervised Domain Adaptation for Nighttime Aerial Tracking in CVPR2022\n    Benchmarks We construct some pioneer benchmarks to serve for the development of nighttime object tracking:   NAT2021‚Äîa pioneering benchmark for unsupervised domain adaptive nighttime tracking.     DarkTrack2021‚Äîa nighttime tracking benchmark comprises 110 challenging sequences with 100K frames in total.     UAVDark135‚Äîa pioneering UAV dark tracking benchmark consists of 135 videos with a variety of objects.  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"75df379dbb18a7c99436896c0df12f57","permalink":"https://example.com/project/nighttime_tracking/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/project/nighttime_tracking/","section":"project","summary":"Adapt SOTA tracking approaches to low-light conditions.","tags":["Low-light","Aerial tracking"],"title":"Nighttime Aerial Tracking","type":"project"},{"authors":["Junjie Ye","Âê≥ÊÅ©ÈÅî"],"categories":["Demo","ÊïôÁ®ã"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It‚Äôs a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Tutorial and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy‚Äôs future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you‚Äôll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://example.com/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":null,"categories":null,"content":"UAVDark135 is the very first UAV dark tracking benchmark dedicated to providing a comprehensive evaluation of tracking performance at night.\nUAVDark135 consists of 135 sequences, most of which were shot by a standard UAV at night, including more than 125k manually annotated frames. The benchmark covers a wide range of scenes, e.g., road, ocean, street, highway, and lakeside, including a large number of objects, such as person, car, building, athlete, truck, and bike.\nThe benchmark is available here (password: axci).\nUAVDark135 Tracking Benchmark A. Platform and Statistics Standing as the first UAV dark tracking benchmark, the UAVDark135 contains totally 135 sequences captured by a standard UAV2 at night. The benchmark includes various tracking scenes, e.g., crossings, t-junctions, road, highway, and consists of different kinds of tracked objects like people, boat, bus, car, truck, athletes, house, etc. To extent the covered scenes, the benchmark also contains some sequences from YouTube, which were shot on the sea. The total frames, mean frames, maximum frames, and minimum frames of the benchmark are 125466, 929, 4571, and 216 respectively, making it suitable for large-scale evaluation. The videos are captured at a frame-rate of 30 frames/s (FPS), with the resolution of 1920√ó1080.\nB. Annotation The frames in UAVDark135 are all manually annotated, where a sequence is completely processed by the same annotator to ensure consistency. Since in some dark scenes the object is nearly invisible, annotation process is much more strenuous. After the first round, 5 professional annotators carefully checked the results and made revision for several rounds to reduce errors as much as possible in nearly 2 months.\nSince the boundary contour of the object is not obvious in the dark, the result boxes of the first annotation fluctuates in continuous image frames. However, the actual motion process of the object should be smooth. In these considerations, we record the original annotation every 5 frames for the sequence with extremely severe vibration, and the results of the remaining frames are obtained by linear interpolation, which is closer to the position and scale variation of the real object.\nC. Attributes    For more details, please refer to our paper.\n","date":1604102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604102400,"objectID":"2d77a9e9fdf8af223bcf640d5b739add","permalink":"https://example.com/project/uavdark135/","publishdate":"2020-10-31T00:00:00Z","relpermalink":"/project/uavdark135/","section":"project","summary":"A pioneering UAV dark tracking benchmark consists of 135 videos with a variety of objects.","tags":["UAV dark tracking","Benchmark"],"title":"UAVDark135","type":"project"},{"authors":["Changhong Fu","Junjie Ye","Juntao Xu","Yujie He","Fuling Lin"],"categories":null,"content":"","date":1602028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602028800,"objectID":"06a2c27d3e614c9491702be79f0c1bcf","permalink":"https://example.com/publication/2020_tgrs_ibri-tracker/","publishdate":"2020-10-07T00:00:00Z","relpermalink":"/publication/2020_tgrs_ibri-tracker/","section":"publication","summary":"IEEE T-GRS (IF: 8.2). *Introduced the interval response inconsistency and the disruptor-aware mechanism into correlation filters.*","tags":["Aerial object tracking","Discriminative correlation filter (DCF)","Temporal regularization","Historical frame information","Interval-based response inconsistency","Disruptor-aware bucketing"],"title":"Disruptor-Aware Interval-Based Response Inconsistency for Correlation Filters in Real-Time Aerial Tracking","type":"publication"},{"authors":null,"categories":null,"content":"Overview Construct robust Siamese network-based trackers for high-performance UAV tracking. Through enhancing the anchor proposal process, feature fusion strategy, attention mechanism, etc, we have developed several robust deep learning-based trackers for UAV. Papers with code Related works are presented as follows:\n  Proposed the anchor proposal network (APN) for adaptive anchor proposing. Alleviated the hyper-parameters in anchor-based approaches and redundent anchors in anchor-free approaches simultaneously. \n Siamese Anchor Proposal Network for High-Speed Aerial Tracking in ICRA 2021\n  Onboard Real-Time Aerial Tracking with Efficient Siamese Anchor Proposal Network in IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 2021\n   Integrated self-attention and cross-attention into SiamAPN, enhanced the perception ability for various scale objects of the proposed SiamAPN++. Evaluation on UAV tracking datasets and real-world onboard test demonstrate its effectiveness. \n SiamAPN++: Siamese Attentional Aggregation Network for Real-Time UAV Tracking in IROS 2021\n   Introduced the hierarchical feature transformer into the Siamese framework to achieve interactive fusion of spatial and semantic cues. \n HiFT: Hierarchical Feature Transformer for Aerial Tracking in ICCV 2021\n   Benchmarks    UAMT100‚Äîa benchmark built for UAM tracking method evaluation which contains 100 image sequences recorded on a flying UAM platform.  ","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"00301ba4e98f37690db7fa9d73ed2a9b","permalink":"https://example.com/project/siamesetracking/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/project/siamesetracking/","section":"project","summary":"Construct robust Siamese network-based trackers for high-performance UAV tracking.","tags":["Siamese Network","Aerial tracking"],"title":"Siamese Network-Based UAV Tracking","type":"project"},{"authors":null,"categories":null,"content":"Overview Develop efficient and robust correlation filter (CF)-based trackers on CPU for UAV tracking in challenging scenarios. By mining temporal, spatial, and channel information properly, we have constructed several competitive tracking approaches while maintaining real-time performance on a single CPU.\n Papers with code Related works are presented as follows:\n  Introduced the temporal regularization based on historical interval response inconsistency and the disruptor-aware mechanism based on response bucketing into the CF framework, realizing competitive performance on several UAV tracking-specific benchmarks. \n Disruptor-Aware Interval-Based Response Inconsistency for Correlation Filters in Real-Time Aerial Tracking in IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING\n   Proposed the response deviation-aware regularization and the channel reliability aware regularization to make full use of the crucial information in response maps and reliable channels. Experiments demonstrate the proposed MRCF‚Äôs superiority in numerous challenging UAV tracking scenarios. Further, an original UAV self-localization system based on the proposed tracking approach is constructed. \n Multi-Regularized Correlation Filter for UAV Tracking and Self-Localization in IEEE Transactions on Industrial Electronics\n   Constructed a novel DCF-based tracker to enhance the sensitivity and resistance to mutations with an adaptive hybrid label. Considerable experiments on widely used UAV tracking benchmarks demonstrate its effectiveness. \n Mutation Sensitive Correlation Filter for Real-Time UAV Tracking with Adaptive Hybrid Label in ICRA 2021\n   Proposed a scale-channel attention-based Siamese network for unmanned aerial manipulator (UAM) tracking, along with a pioneering UAM tracking benchmark. \n Siamese Object Tracking for Vision-Based UAM Approaching with Pairwise Scale-Channel Attention in IROS 2022\n      Filmed in my hometown using a UAV  ","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"8a452e33ff898db0e058ada811a8f40d","permalink":"https://example.com/project/cftracking/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/project/cftracking/","section":"project","summary":"Develop efficient and robust trackers on CPU for UAV tracking in challenging scenarios.","tags":["Correlation filter","Aerial tracking"],"title":"Correlation Filter-Based UAV Tracking","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne  Two  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://example.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]